2025-07-05 17:04:10,615 - CustomGigaChat - INFO - Starting invoke  with: [SystemMessage(content='Твоя задача определить уровень знаний пользователя. Ты должен проанализировать ответ пользователя и дать ОДИН ОТВЕТ В ОДНО СЛОВО! Вотдва варианта и критерии для каждого из них:\n    1. "Новичок" - если пользователь говорит что не имеет большого опыта и не уверен в своих знаниях\n    2. "Профи" - если пользователь говорит что опытен и разбирается в теме\n    \n    ', additional_kwargs={}, response_metadata={}), HumanMessage(content='1', additional_kwargs={}, response_metadata={})]
2025-07-05 17:04:10,938 - CustomGigaChat - INFO - request tokens: 109, response tokens: 4, total tokens: 113
2025-07-05 17:04:11,710 - CustomGigaChat - INFO - Starting invoke  with: [SystemMessage(content='Ты помощник для новичков, ты должен отвечать на вопросы максимально понятно, поясняя сложные термины, избегать болшьих формул и пытаться приводить аналогии\n \n            Ты должен отвечать ТОЛЬКО на основе следующего документа:\n            Distilling Step-by-Step! Outperforming Larger Language Models\nwith Less Training Data and Smaller Model Sizes\nCheng-Yu Hsieh1∗, Chun-Liang Li 2, Chih-Kuan Yeh3, Hootan Nakhost 2,\nYasuhisa Fujii3, Alexander Ratner 1, Ranjay Krishna 1, Chen-Yu Lee 2, Tomas Pﬁster 2\n1University of Washington, 2Google Cloud AI Research, 3Google Research\ncydhsieh@cs.washington.edu\nAbstract\nDeploying large language models (LLMs) is\nchallenging because they are memory inef-\nﬁcient and compute-intensive for practical\napplications. In reaction, researchers train\nsmaller task-speciﬁc models by either ﬁnetun-\ning with human labels or distilling using LLM-\ngenerated labels. However, ﬁnetuning and\ndistillation require large amounts of training\ndata to achieve comparable performance to\nLLMs. We introduce Distilling step-by-step ,\na new mechanism that (a) trains smaller mod-\nels that outperform LLMs, and (b) achieves\nso by leveraging less training data needed\nby ﬁnetuning or distillation. Our method\nextracts LLM rationales as additional super-\nvision for small models within a multi-task\ntraining framework. We present three ﬁnd-\nings across 4 NLP benchmarks: First, com-\npared to both ﬁnetuning and distillation, our\nmechanism achieves better performance with\nmuch fewer labeled/unlabeled training exam-\nples. Second, compared to LLMs, we achieve\nbetter performance using substantially smaller\nmodel sizes. Third, we reduce both the model\nsize and the amount of data required to out-\nperform LLMs; our 770M T5 model outper-\nforms the 540B PaLM model using only 80%\nof available data on a benchmark task.\n1 Introduction\nDespite the impressive few-shot ability offered by\nlarge language models (LLMs) (Brown et al., 2020;\nChowdhery et al., 2022; Thoppilan et al., 2022;\nHoffmann et al., 2022; Smith et al., 2022b; Zhang\net al., 2022), these models are challenging to de-\nploy in real world applications due to their sheer\nsize. Serving a single\n175 billion LLM requires\nat least 350GB GPU memory using specialized in-\nfrastructure (Zheng et al., 2022). To make matters\nworse, today’s state-of-the-art LLMs are composed\n∗Work done while the author was a student researcher at\nGoogle Cloud AI Research.\nFigure 1: While large language models (LLMs) offer\nstrong zero/few-shot performance, they are challeng-\ning to serve in practice. Traditional ways of training\nsmall task-speciﬁc models, on the other hand, requires\nlarge amount of training data. We propose Distilling\nstep-by-step, a new paradigm that extracts rationales\nfrom LLMs as informative task knowledge into training\nsmall models, which reduces both the deployed model\nsize as well as the data required for training.\nof over 500B parameters (Chowdhery et al., 2022),\nrequiring signiﬁcantly more memory and compute.\nSuch computational requirements are far beyond\naffordable for most product teams, especially for\napplications that require low latency performance.\nTo circumvent these deployment challenges of\nlarge models, practitioners often choose to de-\nploy smaller specialized models instead. These\nsmaller models are trained using one of two\ncommon paradigms: ﬁnetuning or distillation.\nFinetuning updates a pretrained smaller model\n(e.g. BERT (Devlin et al., 2018) or T5 (Raffel\net al., 2020)) using downstream human annotated\ndata (Howard and Ruder, 2018). Distillation trains\nthe same smaller models with labels generated by\na larger LLM (Tang et al., 2019; Wang et al., 2021;\nSmith et al., 2022a; Arora et al., 2022). Unfortu-\nnately, these paradigms reduce model size at a cost:\nto achieve comparable performance to LLMs, ﬁne-\ntuning requires expensive human labels, and dis-\ntillation requires large amounts of unlabeled data\nwhich can be hard to obtain (Tang et al., 2019;\nLiang et al., 2020).\nIn this work, we introduce Distilling step-by-\nstep, a new simple mechanism for training smaller\narXiv:2305.02301v1  [cs.CL]  3 May 2023\n            Правила:\n            1. Если вопрос не относится к теме документа: "Это вне рамок предоставленных материалов"\n            2. Если требуется отвечать максимально понятно, то ты можешь пояснять термины развёрнуто.\n            3. Не придумывай информацию, которой нет в документе.\n            4. Для сложных вопросов объединяй информацию из разных частей документа\n            5. Учитывай свою роль помощника для новичков или профессионалов\n            ', additional_kwargs={}, response_metadata={}), HumanMessage(content='1', additional_kwargs={}, response_metadata={})]
2025-07-05 17:04:11,942 - CustomGigaChat - INFO - request tokens: 27, response tokens: 16, total tokens: 156
2025-07-05 17:04:12,554 - CustomGigaChat - INFO - Starting invoke  with: [SystemMessage(content='Ты помощник для новичков, ты должен отвечать на вопросы максимально понятно, поясняя сложные термины, избегать болшьих формул и пытаться приводить аналогии\n \n            Ты должен отвечать ТОЛЬКО на основе следующего документа:\n            Distilling Step-by-Step! Outperforming Larger Language Models\nwith Less Training Data and Smaller Model Sizes\nCheng-Yu Hsieh1∗, Chun-Liang Li 2, Chih-Kuan Yeh3, Hootan Nakhost 2,\nYasuhisa Fujii3, Alexander Ratner 1, Ranjay Krishna 1, Chen-Yu Lee 2, Tomas Pﬁster 2\n1University of Washington, 2Google Cloud AI Research, 3Google Research\ncydhsieh@cs.washington.edu\nAbstract\nDeploying large language models (LLMs) is\nchallenging because they are memory inef-\nﬁcient and compute-intensive for practical\napplications. In reaction, researchers train\nsmaller task-speciﬁc models by either ﬁnetun-\ning with human labels or distilling using LLM-\ngenerated labels. However, ﬁnetuning and\ndistillation require large amounts of training\ndata to achieve comparable performance to\nLLMs. We introduce Distilling step-by-step ,\na new mechanism that (a) trains smaller mod-\nels that outperform LLMs, and (b) achieves\nso by leveraging less training data needed\nby ﬁnetuning or distillation. Our method\nextracts LLM rationales as additional super-\nvision for small models within a multi-task\ntraining framework. We present three ﬁnd-\nings across 4 NLP benchmarks: First, com-\npared to both ﬁnetuning and distillation, our\nmechanism achieves better performance with\nmuch fewer labeled/unlabeled training exam-\nples. Second, compared to LLMs, we achieve\nbetter performance using substantially smaller\nmodel sizes. Third, we reduce both the model\nsize and the amount of data required to out-\nperform LLMs; our 770M T5 model outper-\nforms the 540B PaLM model using only 80%\nof available data on a benchmark task.\n1 Introduction\nDespite the impressive few-shot ability offered by\nlarge language models (LLMs) (Brown et al., 2020;\nChowdhery et al., 2022; Thoppilan et al., 2022;\nHoffmann et al., 2022; Smith et al., 2022b; Zhang\net al., 2022), these models are challenging to de-\nploy in real world applications due to their sheer\nsize. Serving a single\n175 billion LLM requires\nat least 350GB GPU memory using specialized in-\nfrastructure (Zheng et al., 2022). To make matters\nworse, today’s state-of-the-art LLMs are composed\n∗Work done while the author was a student researcher at\nGoogle Cloud AI Research.\nFigure 1: While large language models (LLMs) offer\nstrong zero/few-shot performance, they are challeng-\ning to serve in practice. Traditional ways of training\nsmall task-speciﬁc models, on the other hand, requires\nlarge amount of training data. We propose Distilling\nstep-by-step, a new paradigm that extracts rationales\nfrom LLMs as informative task knowledge into training\nsmall models, which reduces both the deployed model\nsize as well as the data required for training.\nof over 500B parameters (Chowdhery et al., 2022),\nrequiring signiﬁcantly more memory and compute.\nSuch computational requirements are far beyond\naffordable for most product teams, especially for\napplications that require low latency performance.\nTo circumvent these deployment challenges of\nlarge models, practitioners often choose to de-\nploy smaller specialized models instead. These\nsmaller models are trained using one of two\ncommon paradigms: ﬁnetuning or distillation.\nFinetuning updates a pretrained smaller model\n(e.g. BERT (Devlin et al., 2018) or T5 (Raffel\net al., 2020)) using downstream human annotated\ndata (Howard and Ruder, 2018). Distillation trains\nthe same smaller models with labels generated by\na larger LLM (Tang et al., 2019; Wang et al., 2021;\nSmith et al., 2022a; Arora et al., 2022). Unfortu-\nnately, these paradigms reduce model size at a cost:\nto achieve comparable performance to LLMs, ﬁne-\ntuning requires expensive human labels, and dis-\ntillation requires large amounts of unlabeled data\nwhich can be hard to obtain (Tang et al., 2019;\nLiang et al., 2020).\nIn this work, we introduce Distilling step-by-\nstep, a new simple mechanism for training smaller\narXiv:2305.02301v1  [cs.CL]  3 May 2023\n            Правила:\n            1. Если вопрос не относится к теме документа: "Это вне рамок предоставленных материалов"\n            2. Если требуется отвечать максимально понятно, то ты можешь пояснять термины развёрнуто.\n            3. Не придумывай информацию, которой нет в документе.\n            4. Для сложных вопросов объединяй информацию из разных частей документа\n            5. Учитывай свою роль помощника для новичков или профессионалов\n            ', additional_kwargs={}, response_metadata={}), HumanMessage(content='1', additional_kwargs={}, response_metadata={}), HumanMessage(content='1', additional_kwargs={}, response_metadata={})]
2025-07-05 17:04:13,202 - CustomGigaChat - INFO - request tokens: 29, response tokens: 89, total tokens: 274
2025-07-05 17:04:13,847 - CustomGigaChat - INFO - Starting invoke  with: [SystemMessage(content='Ты помощник для новичков, ты должен отвечать на вопросы максимально понятно, поясняя сложные термины, избегать болшьих формул и пытаться приводить аналогии\n \n            Ты должен отвечать ТОЛЬКО на основе следующего документа:\n            Distilling Step-by-Step! Outperforming Larger Language Models\nwith Less Training Data and Smaller Model Sizes\nCheng-Yu Hsieh1∗, Chun-Liang Li 2, Chih-Kuan Yeh3, Hootan Nakhost 2,\nYasuhisa Fujii3, Alexander Ratner 1, Ranjay Krishna 1, Chen-Yu Lee 2, Tomas Pﬁster 2\n1University of Washington, 2Google Cloud AI Research, 3Google Research\ncydhsieh@cs.washington.edu\nAbstract\nDeploying large language models (LLMs) is\nchallenging because they are memory inef-\nﬁcient and compute-intensive for practical\napplications. In reaction, researchers train\nsmaller task-speciﬁc models by either ﬁnetun-\ning with human labels or distilling using LLM-\ngenerated labels. However, ﬁnetuning and\ndistillation require large amounts of training\ndata to achieve comparable performance to\nLLMs. We introduce Distilling step-by-step ,\na new mechanism that (a) trains smaller mod-\nels that outperform LLMs, and (b) achieves\nso by leveraging less training data needed\nby ﬁnetuning or distillation. Our method\nextracts LLM rationales as additional super-\nvision for small models within a multi-task\ntraining framework. We present three ﬁnd-\nings across 4 NLP benchmarks: First, com-\npared to both ﬁnetuning and distillation, our\nmechanism achieves better performance with\nmuch fewer labeled/unlabeled training exam-\nples. Second, compared to LLMs, we achieve\nbetter performance using substantially smaller\nmodel sizes. Third, we reduce both the model\nsize and the amount of data required to out-\nperform LLMs; our 770M T5 model outper-\nforms the 540B PaLM model using only 80%\nof available data on a benchmark task.\n1 Introduction\nDespite the impressive few-shot ability offered by\nlarge language models (LLMs) (Brown et al., 2020;\nChowdhery et al., 2022; Thoppilan et al., 2022;\nHoffmann et al., 2022; Smith et al., 2022b; Zhang\net al., 2022), these models are challenging to de-\nploy in real world applications due to their sheer\nsize. Serving a single\n175 billion LLM requires\nat least 350GB GPU memory using specialized in-\nfrastructure (Zheng et al., 2022). To make matters\nworse, today’s state-of-the-art LLMs are composed\n∗Work done while the author was a student researcher at\nGoogle Cloud AI Research.\nFigure 1: While large language models (LLMs) offer\nstrong zero/few-shot performance, they are challeng-\ning to serve in practice. Traditional ways of training\nsmall task-speciﬁc models, on the other hand, requires\nlarge amount of training data. We propose Distilling\nstep-by-step, a new paradigm that extracts rationales\nfrom LLMs as informative task knowledge into training\nsmall models, which reduces both the deployed model\nsize as well as the data required for training.\nof over 500B parameters (Chowdhery et al., 2022),\nrequiring signiﬁcantly more memory and compute.\nSuch computational requirements are far beyond\naffordable for most product teams, especially for\napplications that require low latency performance.\nTo circumvent these deployment challenges of\nlarge models, practitioners often choose to de-\nploy smaller specialized models instead. These\nsmaller models are trained using one of two\ncommon paradigms: ﬁnetuning or distillation.\nFinetuning updates a pretrained smaller model\n(e.g. BERT (Devlin et al., 2018) or T5 (Raffel\net al., 2020)) using downstream human annotated\ndata (Howard and Ruder, 2018). Distillation trains\nthe same smaller models with labels generated by\na larger LLM (Tang et al., 2019; Wang et al., 2021;\nSmith et al., 2022a; Arora et al., 2022). Unfortu-\nnately, these paradigms reduce model size at a cost:\nto achieve comparable performance to LLMs, ﬁne-\ntuning requires expensive human labels, and dis-\ntillation requires large amounts of unlabeled data\nwhich can be hard to obtain (Tang et al., 2019;\nLiang et al., 2020).\nIn this work, we introduce Distilling step-by-\nstep, a new simple mechanism for training smaller\narXiv:2305.02301v1  [cs.CL]  3 May 2023\n            Правила:\n            1. Если вопрос не относится к теме документа: "Это вне рамок предоставленных материалов"\n            2. Если требуется отвечать максимально понятно, то ты можешь пояснять термины развёрнуто.\n            3. Не придумывай информацию, которой нет в документе.\n            4. Для сложных вопросов объединяй информацию из разных частей документа\n            5. Учитывай свою роль помощника для новичков или профессионалов\n            ', additional_kwargs={}, response_metadata={}), HumanMessage(content='1', additional_kwargs={}, response_metadata={}), HumanMessage(content='1', additional_kwargs={}, response_metadata={}), HumanMessage(content='1', additional_kwargs={}, response_metadata={})]
2025-07-05 17:04:14,000 - CustomGigaChat - INFO - request tokens: 31, response tokens: 10, total tokens: 315
2025-07-05 17:04:14,702 - CustomGigaChat - INFO - Starting invoke  with: [SystemMessage(content='Ты помощник для новичков, ты должен отвечать на вопросы максимально понятно, поясняя сложные термины, избегать болшьих формул и пытаться приводить аналогии\n \n            Ты должен отвечать ТОЛЬКО на основе следующего документа:\n            Distilling Step-by-Step! Outperforming Larger Language Models\nwith Less Training Data and Smaller Model Sizes\nCheng-Yu Hsieh1∗, Chun-Liang Li 2, Chih-Kuan Yeh3, Hootan Nakhost 2,\nYasuhisa Fujii3, Alexander Ratner 1, Ranjay Krishna 1, Chen-Yu Lee 2, Tomas Pﬁster 2\n1University of Washington, 2Google Cloud AI Research, 3Google Research\ncydhsieh@cs.washington.edu\nAbstract\nDeploying large language models (LLMs) is\nchallenging because they are memory inef-\nﬁcient and compute-intensive for practical\napplications. In reaction, researchers train\nsmaller task-speciﬁc models by either ﬁnetun-\ning with human labels or distilling using LLM-\ngenerated labels. However, ﬁnetuning and\ndistillation require large amounts of training\ndata to achieve comparable performance to\nLLMs. We introduce Distilling step-by-step ,\na new mechanism that (a) trains smaller mod-\nels that outperform LLMs, and (b) achieves\nso by leveraging less training data needed\nby ﬁnetuning or distillation. Our method\nextracts LLM rationales as additional super-\nvision for small models within a multi-task\ntraining framework. We present three ﬁnd-\nings across 4 NLP benchmarks: First, com-\npared to both ﬁnetuning and distillation, our\nmechanism achieves better performance with\nmuch fewer labeled/unlabeled training exam-\nples. Second, compared to LLMs, we achieve\nbetter performance using substantially smaller\nmodel sizes. Third, we reduce both the model\nsize and the amount of data required to out-\nperform LLMs; our 770M T5 model outper-\nforms the 540B PaLM model using only 80%\nof available data on a benchmark task.\n1 Introduction\nDespite the impressive few-shot ability offered by\nlarge language models (LLMs) (Brown et al., 2020;\nChowdhery et al., 2022; Thoppilan et al., 2022;\nHoffmann et al., 2022; Smith et al., 2022b; Zhang\net al., 2022), these models are challenging to de-\nploy in real world applications due to their sheer\nsize. Serving a single\n175 billion LLM requires\nat least 350GB GPU memory using specialized in-\nfrastructure (Zheng et al., 2022). To make matters\nworse, today’s state-of-the-art LLMs are composed\n∗Work done while the author was a student researcher at\nGoogle Cloud AI Research.\nFigure 1: While large language models (LLMs) offer\nstrong zero/few-shot performance, they are challeng-\ning to serve in practice. Traditional ways of training\nsmall task-speciﬁc models, on the other hand, requires\nlarge amount of training data. We propose Distilling\nstep-by-step, a new paradigm that extracts rationales\nfrom LLMs as informative task knowledge into training\nsmall models, which reduces both the deployed model\nsize as well as the data required for training.\nof over 500B parameters (Chowdhery et al., 2022),\nrequiring signiﬁcantly more memory and compute.\nSuch computational requirements are far beyond\naffordable for most product teams, especially for\napplications that require low latency performance.\nTo circumvent these deployment challenges of\nlarge models, practitioners often choose to de-\nploy smaller specialized models instead. These\nsmaller models are trained using one of two\ncommon paradigms: ﬁnetuning or distillation.\nFinetuning updates a pretrained smaller model\n(e.g. BERT (Devlin et al., 2018) or T5 (Raffel\net al., 2020)) using downstream human annotated\ndata (Howard and Ruder, 2018). Distillation trains\nthe same smaller models with labels generated by\na larger LLM (Tang et al., 2019; Wang et al., 2021;\nSmith et al., 2022a; Arora et al., 2022). Unfortu-\nnately, these paradigms reduce model size at a cost:\nto achieve comparable performance to LLMs, ﬁne-\ntuning requires expensive human labels, and dis-\ntillation requires large amounts of unlabeled data\nwhich can be hard to obtain (Tang et al., 2019;\nLiang et al., 2020).\nIn this work, we introduce Distilling step-by-\nstep, a new simple mechanism for training smaller\narXiv:2305.02301v1  [cs.CL]  3 May 2023\n            Правила:\n            1. Если вопрос не относится к теме документа: "Это вне рамок предоставленных материалов"\n            2. Если требуется отвечать максимально понятно, то ты можешь пояснять термины развёрнуто.\n            3. Не придумывай информацию, которой нет в документе.\n            4. Для сложных вопросов объединяй информацию из разных частей документа\n            5. Учитывай свою роль помощника для новичков или профессионалов\n            ', additional_kwargs={}, response_metadata={}), HumanMessage(content='1', additional_kwargs={}, response_metadata={}), HumanMessage(content='1', additional_kwargs={}, response_metadata={}), HumanMessage(content='1', additional_kwargs={}, response_metadata={}), HumanMessage(content='1', additional_kwargs={}, response_metadata={})]
2025-07-05 17:04:14,863 - CustomGigaChat - INFO - request tokens: 33, response tokens: 10, total tokens: 358
